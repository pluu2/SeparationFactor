### SeparationFactor
----

K-sparse Conditional-Variational Autoencoder implementation. 

Variational Autoencoders(VAEs) map data 'x' into a latent dimension 'z'. The dimension of 'z' is typically much smaller than 'x',  As VAEs learns the distribution of x, it is believed that the underlying features of the distribution can be implicitely learned. The hope with VAEs is that with proper training the latent space will come to reflect the underlying features of the incoming data. Unfortunately this is not exactly the case. 

There is signficant work being done on modifying VAEs in such a way to improve how well these features are mapped on to the latent space 'z'. This mechanism is known as 'disentanglment'. A perfectly disentangled latent representation is when modification of a single latent dimension will modify a single factor generated by the VAE [1] . Unfortunately this definition is vague, and has been a topic of extensive debate [2] . 

Current methods on attempting to improve unsupervised disentanglement have focused primarily on using alternative liklihood functions[3][4].

Another avenue of investigation involves supervised disnetanglement, which focuses on modifying the liklihood function of VAEs, by implementing an inductive bias on the posterior. This bias comes in the form of an additional condition on the liklihood function (p(x|z,c)). Specifically by adding a classifier to the encoder, one can add 'structure' to the posterior distribution [5]. 

To further improve this structure, I have written a custom layer for Tensorflow which can be added between a classifier and a latent variable to boost the separation between different latent variables. I originally named this 'separation factor', but I found this later to be known as K-Sparse [6]. 

To see the implementation I have created a notebook with the basic use of my 'separation factor' 
<a href = "https://github.com/pluu2/SeparationFactor/blob/master/Conditional_VAEs_with_K_Sparse.ipynb"> Basic Implementation</a> 






References:
----

[1] https://towardsdatascience.com/disentanglement-with-variational-autoencoder-a-review-653a891b69bd
[2]https://arxiv.org/abs/1811.12359 
[3]https://arxiv.org/abs/1606.04934
[4]https://arxiv.org/pdf/1611.05013.pdf
[5]Kihyuk,S., Lee, H., Yan, X. (2015) Learning Structured Output Representation using Deep Conditional Generative Models. Advances in Neural Information Processing Systems.
[6]Makhazani,A. Frey,B. (2013). K-Sparse Autoencoders. arXiv: 1312.5663

